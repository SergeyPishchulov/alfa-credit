{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbfbb3a2",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae00d53-9276-417e-a92d-d7bc862248be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def seed_everything(seed=24535):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986d3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "    f'data_for_competition/train_data/train_data_{i}.pq' for i in range(12)]\n",
    "# file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b2edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_tensor(df, orders_cnt=70):\n",
    "    tensors = []\n",
    "    ids = []\n",
    "    for id, order in df.groupby(\"id\"):\n",
    "        order = order.sort_values(\"rn\").drop('rn', axis=1).values\n",
    "        tensors.append(torch.tensor(\n",
    "            np.pad(order, [(0, orders_cnt-order.shape[0]), (0, 0)], mode='constant')))\n",
    "        # ids.append(id)\n",
    "    return torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91b8456a",
   "metadata": {},
   "source": [
    "Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c82173be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, filenames, sample):\n",
    "      # `filenames` is a list of strings that contains all file names.\n",
    "      # `batch_size` determines the number of files that we want to read in a chunk.\n",
    "        self.filenames = filenames\n",
    "        self.sample = sample\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of chunks.\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):  # idx means index of the chunk.\n",
    "      # In this method, we do all the preprocessing.\n",
    "      # First read  data from files in a chunk. Preprocess it. Extract labels. Then return data and labels.\n",
    "        # This extracts one batch of file names from the list `filenames`.\n",
    "        file = self.filenames[idx]\n",
    "        data = pd.read_parquet(file, engine='fastparquet')\n",
    "        cat_columns = list(data.columns)[2:]\n",
    "        data[cat_columns] = data[cat_columns].astype('category')\n",
    "\n",
    "        # ids, data = get_padded_tensor(data)\n",
    "\n",
    "        target_file_name = f'data_for_competition/{self.sample}_target.csv'\n",
    "        target = pd.read_csv(target_file_name)\n",
    "        labels = target[target.id.isin(set(data.id))]\n",
    "        # The following condition is actually needed in Pytorch. Otherwise, for our particular example, the iterator will be an infinite loop.\n",
    "        # Readers can verify this by removing this condition.\n",
    "        if idx == self.__len__():\n",
    "            raise IndexError\n",
    "\n",
    "        return data, labels\n",
    "    \n",
    "def get_batches(inputs, labels, batch_size=10_000):\n",
    "    \"\"\"batch_size - кол-во продуктов-записей\"\"\"\n",
    "    if len(inputs) <= batch_size:\n",
    "        return [inputs], [labels]\n",
    "    res_batches = []\n",
    "    res_labels = []\n",
    "    l = 0\n",
    "    last_id = -1\n",
    "    while last_id != labels.id.iloc[-1]:\n",
    "        next_id_ind = inputs[inputs.id > last_id].index[0]\n",
    "        l = inputs.index.get_loc(next_id_ind)\n",
    "        first_id = inputs.iloc[l].id\n",
    "        if l+batch_size >= len(inputs):\n",
    "            last_id = inputs.id.iloc[-1]\n",
    "        else:\n",
    "            last_id = inputs.iloc[l+batch_size].id\n",
    "        batch = inputs[(inputs.id >= first_id) & (inputs.id <= last_id)]\n",
    "        res_batches.append(batch)\n",
    "        res_labels.append(labels[labels.id.isin(set(batch.id))])\n",
    "    return res_batches, res_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d267f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = CustomDataset(\n",
    "    [f'data_for_competition/train_data/train_data_{i}.pq' for i in range(11)],  'train')\n",
    "val_dataloader = CustomDataset(\n",
    "    [f'data_for_competition/train_data/train_data_{11}.pq'], 'val')\n",
    "test_dataloader = CustomDataset([f'data_for_competition/test_data/test_data_{i}.pq' for i in range(2)],\n",
    "                                'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5756e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = train_dataloader[0]\n",
    "b_inp, b_lab = get_batches(inputs, labels, batch_size=10_000)\n",
    "batch = b_inp[0]\n",
    "label = b_lab[0]\n",
    "tensor = get_padded_tensor(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0efc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mini_sample = inputs[:100_000]\n",
    "# for c in inputs.columns[1:]:\n",
    "#     fig = go.Figure()\n",
    "#     fig.update_layout(title = c)\n",
    "#     fig.add_trace(go.Histogram(x=mini_sample[c],\n",
    "#                                 histnorm='probability'))\n",
    "#     fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "686dc280",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9f87211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class LstmClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LstmClassifier, self).__init__()\n",
    "        self.num_classes = num_classes  # number of classes\n",
    "        self.num_layers = num_layers  # number of layers\n",
    "        self.input_size = input_size  # input size\n",
    "        self.hidden_size = hidden_size  # hidden state\n",
    "        self.seq_length = seq_length  # sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)  # lstm\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)  # fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes)  # fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0),\n",
    "                       self.hidden_size))  # hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0),\n",
    "                       self.hidden_size))  # internal state\n",
    "        # Propagate input through LSTM\n",
    "        # lstm with input, hidden, and internal state\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "        # reshaping the data for Dense layer next\n",
    "        hn = hn.view(-1, self.hidden_size)\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out)  # first Dense\n",
    "        out = self.relu(out)  # relu\n",
    "        out = self.fc(out)  # Final Output\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25191bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1270, 70, 60])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26b567d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmClassifier(num_classes=2,\n",
    "                       input_size=tensor.shape[2],# n_features == 60\n",
    "                       hidden_size=2,\n",
    "                       num_layers=1,\n",
    "                       seq_length=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21880bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")#TODO delete\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fed25292",
   "metadata": {},
   "source": [
    "train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f64ac166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss, optimizer, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}:'.format(epoch, num_epochs - 1), flush=True)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                dataloader = train_dataloader\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                dataloader = val_dataloader\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "\n",
    "            # Iterate over data.\n",
    "            for huge_inputs, huge_labels in tqdm(dataloader):\n",
    "                batch_inputs, batch_labels = get_batches(\n",
    "                    huge_inputs, huge_labels, batch_size=10_000)\n",
    "                batch_cnt = 1  # TODO delete\n",
    "                for inputs, labels in zip(batch_inputs[:batch_cnt], batch_labels[:batch_cnt]):\n",
    "                    inputs = get_padded_tensor(inputs).type(torch.float)\n",
    "                    #TODO one hot encode enc_loans_credit_status\n",
    "                    labels = torch.tensor(\n",
    "                        labels.flag.values).type(torch.LongTensor)\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward and backward\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        preds = model(inputs)\n",
    "                        loss_value = loss(preds, labels)\n",
    "                        preds_class = preds.argmax(dim=1)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss_value.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss_value.item()\n",
    "                    running_acc += (preds_class == labels.data).float().mean()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            epoch_acc = running_acc / len(dataloader)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc), flush=True)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efcf4298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serhio/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      " 18%|█▊        | 2/11 [01:50<08:22, 55.82s/it]"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\n",
    "train_model(model,\n",
    "            torch.nn.CrossEntropyLoss(),\n",
    "            optimizer,\n",
    "            torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1),\n",
    "            1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37ace6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
