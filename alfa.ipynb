{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbfbb3a2",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae00d53-9276-417e-a92d-d7bc862248be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "# import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "import shutil \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1460bc-5acf-48c2-8e79-dbb42f3bdcf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.read_csv(\"data_for_competition/test_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986d3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [f'data_for_competition/train_data/train_data_{i}.pq' for i in range(12)]\n",
    "# file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969d782e-ba19-4b46-9d05-f19df9a5bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s=0\n",
    "# for i in range(12):\n",
    "#     df = pd.read_parquet(f'data_for_competition/train_data/train_data_{i}.pq', engine='fastparquet')#[:100]\n",
    "#     s+=df.shape[0]\n",
    "#     print(df.groupby(\"id\").rn.count().max())\n",
    "# # print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d477ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "95    11\n",
       "96    11\n",
       "97    12\n",
       "98    12\n",
       "99    12\n",
       "Name: id, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(f'data_for_competition/train_data/train_data_{0}.pq', engine='fastparquet')[:100]\n",
    "df.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29403402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1.,  1.,  1.,  1.,  1.],\n",
    "              [1.,  1.,  1.,  1.,  1.],\n",
    "              [1.,  1.,  1.,  1.,  1.]])\n",
    "np.pad(a, [(0, 10-a.shape[0]), (0, 0)], mode='constant')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68b2edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_tensor(df, orders_cnt=70):\n",
    "    tensors = []\n",
    "    ids =[]\n",
    "    for id, order in df.groupby(\"id\"):\n",
    "        order = order.sort_values(\"rn\").drop('rn', axis=1).values\n",
    "        tensors.append(torch.tensor(np.pad(order, [(0, orders_cnt-order.shape[0]), (0, 0)], mode='constant')))\n",
    "        # ids.append(id)\n",
    "    return torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6f0fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d2350a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91b8456a",
   "metadata": {},
   "source": [
    "Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116cbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82173be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, filenames, sample):\n",
    "      # `filenames` is a list of strings that contains all file names.\n",
    "      # `batch_size` determines the number of files that we want to read in a chunk.\n",
    "        self.filenames = filenames\n",
    "        self.sample = sample\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of chunks.\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):  # idx means index of the chunk.\n",
    "      # In this method, we do all the preprocessing.\n",
    "      # First read  data from files in a chunk. Preprocess it. Extract labels. Then return data and labels.\n",
    "        # This extracts one batch of file names from the list `filenames`.\n",
    "        file = self.filenames[idx]\n",
    "        data = pd.read_parquet(file, engine='fastparquet')\n",
    "        cat_columns = list(data.columns)[2:]\n",
    "        data[cat_columns] = data[cat_columns].astype('category')\n",
    "\n",
    "        # ids, data = get_padded_tensor(data)\n",
    "\n",
    "        target_file_name = f'data_for_competition/{self.sample}_target.csv'\n",
    "        target = pd.read_csv(target_file_name)\n",
    "        labels = target[target.id.isin(set(data.id))]\n",
    "        # The following condition is actually needed in Pytorch. Otherwise, for our particular example, the iterator will be an infinite loop.\n",
    "        # Readers can verify this by removing this condition.\n",
    "        if idx == self.__len__():\n",
    "            raise IndexError\n",
    "\n",
    "        return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d267f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = CustomDataset([f'data_for_competition/train_data/train_data_{i}.pq' for i in range(12)],\n",
    "                                  'train')\n",
    "test_dataloader = CustomDataset([f'data_for_competition/test_data/test_data_{i}.pq' for i in range(2)],\n",
    "                                  'test')\n",
    "val_dataloader = test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0a2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data, labels in ds:\n",
    "#     print(data.shape)\n",
    "#     # print(labels.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "705c5ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels  = train_dataloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97b28072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "rn                         4\n",
       "pre_since_opened           4\n",
       "pre_since_confirmed        1\n",
       "pre_pterm                  9\n",
       "                          ..\n",
       "enc_loans_credit_status    3\n",
       "enc_loans_credit_type      1\n",
       "enc_loans_account_cur      1\n",
       "pclose_flag                0\n",
       "fclose_flag                0\n",
       "Name: 3, Length: 61, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91dd691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(inputs, labels, batch_size=10_000):\n",
    "    \"\"\"batch_size - кол-во продуктов-записей\"\"\"\n",
    "    if len(inputs) <= batch_size:\n",
    "        return [inputs], [labels]\n",
    "    res_batches = []\n",
    "    res_labels = []\n",
    "    l = 0\n",
    "    last_id =-1\n",
    "    while last_id != labels.id.iloc[-1]:\n",
    "        next_id_ind = inputs[inputs.id > last_id].index[0]\n",
    "        l = inputs.index.get_loc(next_id_ind)\n",
    "        first_id = inputs.iloc[l].id\n",
    "        if l+batch_size>=len(inputs):\n",
    "            last_id = inputs.id.iloc[-1]\n",
    "        else:\n",
    "            last_id = inputs.iloc[l+batch_size].id\n",
    "        batch = inputs[(inputs.id >= first_id) & (inputs.id <= last_id)]\n",
    "        res_batches.append(batch)\n",
    "        res_labels.append(labels[labels.id.isin(set(batch.id))])        \n",
    "    return res_batches, res_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58cb8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5756e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_inp, b_lab = get_batches(inputs, labels , batch_size=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db156dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=b_inp[0]\n",
    "label = b_lab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3edb9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = get_padded_tensor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "51a6ee01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9f87211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable \n",
    "\n",
    "class LstmClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LstmClassifier, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0abb83a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1270, 70, 60)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "26b567d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmClassifier(num_classes=2,\n",
    "                       input_size=tensor.shape[2],\n",
    "                       hidden_size=2,\n",
    "                       num_layers=1,\n",
    "                       seq_length=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "21880bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f64ac166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss, optimizer, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}:'.format(epoch, num_epochs - 1), flush=True)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                dataloader = train_dataloader\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                dataloader = val_dataloader\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "\n",
    "            # Iterate over data.\n",
    "            for huge_inputs, huge_labels in tqdm(dataloader[:1]):\n",
    "                batch_inputs, batch_labels = get_batches(\n",
    "                    huge_inputs, huge_labels, batch_size=10_000)\n",
    "                batch_cnt = 1\n",
    "                for inputs, labels in zip(batch_inputs[:batch_cnt], batch_labels[:batch_cnt]):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward and backward\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        preds = model(inputs)\n",
    "                        loss_value = loss(preds, labels)\n",
    "                        preds_class = preds.argmax(dim=1)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss_value.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss_value.item()\n",
    "                    running_acc += (preds_class == labels.data).float().mean()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            epoch_acc = running_acc / len(dataloader)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc), flush=True)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcf4298",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1.0e-3\u001b[39m)\n\u001b[1;32m      2\u001b[0m train_model(model,\n\u001b[1;32m      3\u001b[0m             torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss(),\n\u001b[1;32m      4\u001b[0m             optimizer,\n\u001b[1;32m      5\u001b[0m             torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer, step_size\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m),\n\u001b[1;32m      6\u001b[0m             \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\n",
    "train_model(model,\n",
    "            torch.nn.CrossEntropyLoss(),\n",
    "            optimizer,\n",
    "            torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1),\n",
    "            1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e188e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
